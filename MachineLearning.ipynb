{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Importing numpy and functions\n",
    "import numpy as np\n",
    "import helpers as h\n",
    "from implementations import *\n",
    "from max_implementations import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Load test module for sanity check\n",
    "#from test_utils import test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from helpers import sample_data, load_data, standardize\n",
    "\n",
    "# load data.\n",
    "height, weight, gender = load_data()\n",
    "\n",
    "# build sampled x and y.\n",
    "seed = 1\n",
    "y = np.expand_dims(gender, axis=1)\n",
    "X = np.c_[height.reshape(-1), weight.reshape(-1)]\n",
    "y, X = sample_data(y, X, seed, size_samples=200)\n",
    "tx, mean_x, std_x = standardize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Loading the data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m X, xHeader, Y, yHeader, indexedX, indexedXheader, indexedY, indexedYheader \u001b[38;5;241m=\u001b[39m \u001b[43mloadTrainingData\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Cleaning/feature engineering the data\u001b[39;00m\n",
      "File \u001b[0;32m~/ml-project-1-andermaxjohannes/implementations.py:360\u001b[0m, in \u001b[0;36mloadTrainingData\u001b[0;34m()\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloadTrainingData\u001b[39m():\n\u001b[1;32m    356\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m''' Loads the medical training data and nothing else. Wrapper function\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;124;03m        X, xHeader, Y, yHeader, indexedX, indexedXheader, indexedY, indexedYheader\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m     x, xHeader \u001b[38;5;241m=\u001b[39m \u001b[43mloadData\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./Data/x_train.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m     y, yHeader \u001b[38;5;241m=\u001b[39m loadData(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./Data/y_train.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    362\u001b[0m     y[y \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/ml-project-1-andermaxjohannes/implementations.py:351\u001b[0m, in \u001b[0;36mloadData\u001b[0;34m(dataPath)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloadData\u001b[39m(dataPath):\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m''' Loads data and returns it as masked numpy array. A masked array contains information about which values are invalid, ensuring methods like .mean() ignores the masked values\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03m        dataPath: The file path of the data\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m        header: (d,) array with the column names\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenfromtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataPath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_header\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musemask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Loading the data as a masked array (with usemask=True), skipping the header, and specifying that the values are floats\u001b[39;00m\n\u001b[1;32m    352\u001b[0m     header \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mgenfromtxt(dataPath, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m, max_rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# Loading the first row of the csv file, i.e. the header\u001b[39;00m\n\u001b[1;32m    353\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data , header\n",
      "File \u001b[0;32m/opt/jlab-env/lib/python3.11/site-packages/numpy/lib/npyio.py:2245\u001b[0m, in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding, ndmin, like)\u001b[0m\n\u001b[1;32m   2243\u001b[0m \u001b[38;5;66;03m# Parse each line\u001b[39;00m\n\u001b[1;32m   2244\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (i, line) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(itertools\u001b[38;5;241m.\u001b[39mchain([first_line, ], fhd)):\n\u001b[0;32m-> 2245\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43msplit_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2246\u001b[0m     nbvalues \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(values)\n\u001b[1;32m   2247\u001b[0m     \u001b[38;5;66;03m# Skip an empty line\u001b[39;00m\n",
      "File \u001b[0;32m/opt/jlab-env/lib/python3.11/site-packages/numpy/lib/_iotools.py:226\u001b[0m, in \u001b[0;36mLineSplitter.__call__\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, line):\n\u001b[0;32m--> 226\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handyman\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_decode_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jlab-env/lib/python3.11/site-packages/numpy/lib/_iotools.py:205\u001b[0m, in \u001b[0;36mLineSplitter._delimited_splitter\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelimiter)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loading the data\n",
    "X, xHeader, Y, yHeader, indexedX, indexedXheader, indexedY, indexedYheader = loadTrainingData()\n",
    "print('')\n",
    "# Cleaning/feature engineering the data\n",
    "yClean, xClean, xHeaderClean, removedFeatures = dataCleaning(Y,X,xHeader)\n",
    "print('')\n",
    "# Making a balanced data set to force the model to not just predict negatively all the time\n",
    "yBalanced, xBalanced, balancePrior = balanceData(yClean,xClean)\n",
    "print('')\n",
    "# Adding dummy variables and replacing the remaining invalid values by the mean\n",
    "tx = makeTrainingData(xBalanced)\n",
    "print(f'The resultant dataarray tx has shape {tx.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m lambda_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     15\u001b[0m setLambdaReg \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m y,tx,lambda_,initial_w,max_iter,gamma: reg_logistic_regression(y, tx, lambda_, initial_w ,max_iter, gamma)\n\u001b[0;32m---> 17\u001b[0m loss_tr_arr,loss_te_arr,loss_testing_training \u001b[38;5;241m=\u001b[39m \u001b[43mmax_cross_valid\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43minitial_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m,\u001b[49m\u001b[43mregressionFunction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreg_logistic_regression\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#max_cross_valid(y, x, k_fold, initial_w,max_iter,gamma, regressionFunction=reg_logistic_regression,lossFunction=logistic_loss)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#w_logistic_j, loss_logistic_j = k_fold_cross_validation(y,tx,K,initial_w,max_iter,gamma,setLambdaReg)\u001b[39;00m\n",
      "File \u001b[0;32m~/ml-project-1-andermaxjohannes/max_implementations.py:337\u001b[0m, in \u001b[0;36mmax_cross_valid\u001b[0;34m(y, x, k_fold, initial_w, max_iter, gamma, lambda_, regressionFunction, lossFunction)\u001b[0m\n\u001b[1;32m    335\u001b[0m loss_te_arr \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k_fold):\n\u001b[0;32m--> 337\u001b[0m     k_indices \u001b[38;5;241m=\u001b[39m \u001b[43mmax_k_fold_cross_valid_sets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_fold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     \u001b[38;5;66;03m#print(np.shape(k_indices))\u001b[39;00m\n\u001b[1;32m    339\u001b[0m     test_ind \u001b[38;5;241m=\u001b[39m k_indices[k]\n",
      "File \u001b[0;32m~/ml-project-1-andermaxjohannes/max_implementations.py:307\u001b[0m, in \u001b[0;36mmax_k_fold_cross_valid_sets\u001b[0;34m(y, k_fold)\u001b[0m\n\u001b[1;32m    305\u001b[0m interval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(num_row \u001b[38;5;241m/\u001b[39m k_fold)\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m#np.random.seed(seed)\u001b[39;00m\n\u001b[0;32m--> 307\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mpermutation(num_row)\n\u001b[1;32m    308\u001b[0m k_indices \u001b[38;5;241m=\u001b[39m [indices[k \u001b[38;5;241m*\u001b[39m interval : (k \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m interval] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k_fold)]\n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m#print(np.array(k_indices))\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Initializing the parameters at zero and setting some constants\n",
    "initial_w = np.zeros(tx.shape[1])\n",
    "K = 10\n",
    "gamma = 0.5\n",
    "max_iter = 100\n",
    "lambda_ = 0.001\n",
    "# Training a model with gradient descent with momentum\n",
    "#w_gd_m, loss_gd_m = k_fold_cross_validation(yBalanced,tx,K,initial_w,max_iter,gamma, mse_gd_momentum, compute_loss)\n",
    "print('')\n",
    "# Training a model with logistic reg with momentum\n",
    "#w_logistic, loss_logistic = k_fold_cross_validation(yBalanced,tx,K,initial_w,max_iter,gamma)\n",
    "\n",
    "# Training a model with logistic reg with momentum\n",
    "lambda_ = 2\n",
    "setLambdaReg = lambda y,tx,lambda_,initial_w,max_iter,gamma: reg_logistic_regression(y, tx, lambda_, initial_w ,max_iter, gamma)\n",
    "\n",
    "loss_tr_arr,loss_te_arr,loss_testing_training = max_cross_valid(y,tx,K,initial_w,max_iter,gamma,lambda_,regressionFunction=reg_logistic_regression)\n",
    "#max_cross_valid(y, x, k_fold, initial_w,max_iter,gamma, regressionFunction=reg_logistic_regression,lossFunction=logistic_loss)\n",
    "#w_logistic_j, loss_logistic_j = k_fold_cross_validation(y,tx,K,initial_w,max_iter,gamma,setLambdaReg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_testing_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the test data\n",
    "xTest, xIndexedHeader = loadData('./Data/x_test.csv')\n",
    "print(xTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def makePredictions(w,xTest,xHeader,xHeaderFeaturesRemoved, prior=1.0):\n",
    "    ''' Function making predictions based on provided parameters and data\n",
    "    Args:\n",
    "        w: (d,) array with the parameters\n",
    "        x: (N,D) array with the data\n",
    "        xHeader: (D,) array with all the features\n",
    "        xHeader: (d,) array with the features that are actually used\n",
    "        prior: float denoting the probability of a random sample being in the model training data\n",
    "    Returns:\n",
    "        (N,) boolean array of the predictions\n",
    "    '''\n",
    "    standardX = standardizeData(xTest)\n",
    "    removedFeaturesX = standardX[:,np.nonzero(np.isin(xHeader, xHeaderFeaturesRemoved))[0]]\n",
    "    predictionSet = makeTrainingData(removedFeaturesX)\n",
    "    probabilities = prior * logistic(predictionSet@w) # The prob of the model being applicable times the prob from the model\n",
    "    return (np.sign(probabilities-0.5)+1)/2 # Shifting the probs to be negative for negative preds, and vice versa, taking the sign, shifting the preds up to be zero or two, diving by to so the preds are zero or one\n",
    "\"\"\"\n",
    "# Making predictions\n",
    "pred_gd = makePredictions(w_logistic,xTest[:,1:],xHeader,xHeaderClean,balancePrior)\n",
    "pred_logistic = makePredictions(w_logistic,xTest[:,1:],xHeader,xHeaderClean)\n",
    "# Counting predicted positive cases\n",
    "print(np.sum(pred_gd),np.sum(pred_logistic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_logistic[pred_logistic == 0] = -1\n",
    "h.create_csv_submission(xTest[:,0],pred_logistic,'./Predictions/balancedDataNoPrior.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
