{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import helpers as h\n",
    "from implementations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded, there are 321 features and 328135 samples, the shapes of the unindexed data is:\n",
      "y: (328135, 1), x: (328135, 321)\n"
     ]
    }
   ],
   "source": [
    "X, xHeader, Y, yHeader, indexedX, indexedXheader, indexedY, indexedYheader = loadTrainingData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For a threshold of 0.7, there are 144 good features, and 177 bad features\n",
      "There remains in the data 289870 samples with at most 5 missing values\n",
      "The number of invalid entries remaing in the dataset is 272938\n",
      "That is 0.006538802834987332 parts of the whole dataset\n",
      "Removed 21050 samples with outliers more than 10 standard deviations from the mean. There remains 268820 samples in the dataset.\n",
      "Standardized data by subtracting the mean and dividing by the standard deviation\n"
     ]
    }
   ],
   "source": [
    "yClean, xClean, xHeaderClean = dataCleaning(Y,X,xHeader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a balanced subset of the data, with 46448 samples, 23224 each of positive and negative samples\n",
      "Added dummy variable and replaced invalid entries with zeros\n",
      "[1. 0. 0. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "yBalanced, xBalanced = balanceData(yClean,xClean)\n",
    "tx = makeTrainingData(xBalanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation_sets(y,x,K):\n",
    "    ''' Function for making K separate training sets out of the provided dataset\n",
    "    Args:\n",
    "        y: (N,) array of the labels\n",
    "        x: (N,d) array of the data with its features\n",
    "        K: Integer number of separate trainingsets\n",
    "    Yields:\n",
    "        y_k: (N/K,) array of the chosen labels. N/K is N//K + 1 for the first sets, and N//K for the rest of the sets\n",
    "        x_k: (N/K,d) array of the data\n",
    "    '''\n",
    "    N = len(y)      # Saving the number of samples as an integer\n",
    "    batchSize = N // K  # Calculating the batch size\n",
    "    residual = N - K*batchSize  # Checking how many samples would not be included in sets of size N//K\n",
    "\n",
    "    indices = np.random.permutation(N) # Randomly permuted indices of the provided dataset\n",
    "    \n",
    "    for k in range(K):\n",
    "        if k < residual: # If the samples 'in' the residual has not 'been used', we include them\n",
    "            indices_k = indices[k*(batchSize+1):(k+1)*(batchSize+1)] # Indices of the elements for each k batch. Here included one extra samples 'from' the residual\n",
    "        else:\n",
    "            indices_k = indices[residual+k*batchSize:residual+(k+1)*batchSize] # Indices of the elements for each k batch\n",
    "        \n",
    "        yield y[indices_k], x[indices_k] # Yield returns the first set, and next time the function is called the code continues, so the for loop repeats and yields the next set\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1 yielded a loss improvement from 0.5015069967707212 to 0.17084571708603238\n",
      "Run 2 yielded a loss improvement from 0.5005382131324004 to 0.16869899753041984\n",
      "Run 3 yielded a loss improvement from 0.49730893433799783 to 0.17453104590318338\n",
      "Run 4 yielded a loss improvement from 0.5090967811389816 to 0.17350784944791756\n",
      "Run 5 yielded a loss improvement from 0.4915491441489934 to 0.1747786481539316\n",
      "-----------------------------------------------------------------------------------------\n",
      "Averaging the parameters, the loss improves from 0.5 to 0.17387707384674347\n"
     ]
    }
   ],
   "source": [
    "def k_fold_cross_validation(y,tx,K,initial_w,max_iters,gamma, regressionFunction=logistic_regression, lossFunction=logistic_loss):\n",
    "    ''' Performing regression on K separate subsets of the provided training set, and returning the average parameters\n",
    "    Args:\n",
    "        y: (N,) array of the labels\n",
    "        tx: (N,d) array of the data and its features\n",
    "        initital_w: (d,) array with some initialization of the parameters\n",
    "        max_iters: integer of the maximum iterations per regression\n",
    "        gamma: float of the step size\n",
    "        regressionFunction: The function of the chosen type of regression\n",
    "        lossFunction: The function of the chosen type of loss\n",
    "    Returns:\n",
    "        w_avg: (d,) array of the resultant parameters averaged over the cross validation runs\n",
    "    ''' \n",
    "    crossValidationSets = k_fold_cross_validation_sets(y,tx,K)\n",
    "    \n",
    "    w, loss = np.zeros((K,tx.shape[1])), np.zeros(K)\n",
    "    \n",
    "    for k in range(K):\n",
    "        y_k, tx_k = next(crossValidationSets)\n",
    "\n",
    "        w[k], loss[k] = regressionFunction(y_k, tx_k, initial_w, max_iters, gamma)\n",
    "\n",
    "        print(f'Run {k+1} yielded a loss improvement from {lossFunction(y_k,tx_k,initial_w)} to {lossFunction(y_k,tx_k,w[k])}')\n",
    "    w_avg = np.sum(w,axis=0) / K\n",
    "    \n",
    "    print(f'''-----------------------------------------------------------------------------------------\n",
    "Averaging the parameters, the loss improves from {lossFunction(y,tx,initial_w)} to {lossFunction(y,tx,w_avg)}''')\n",
    "    return w_avg\n",
    "\n",
    "initial_w = np.zeros(tx.shape[1])\n",
    "K = 5\n",
    "w = k_fold_cross_validation(yBalanced,tx,K,initial_w,100,0.01, mse_gd_momentum, compute_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "w_gd_m, loss_gd_m = mse_gd_momentum(yBalanced,tx,initial_w,1000,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.1738137728145803\n",
      "0.1738137728145803\n"
     ]
    }
   ],
   "source": [
    "print(compute_loss(yBalanced,tx,initial_w))\n",
    "print(loss_gd_m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "2.3686920114550984\n",
      "0.6931471805600634\n",
      "0.4673017607083231\n"
     ]
    }
   ],
   "source": [
    "w_logistic, loss_logistic = logistic_regression(yBalanced,tx,initial_w,100,0.2)\n",
    "\n",
    "print(compute_loss(yBalanced,tx,initial_w))\n",
    "print(compute_loss(yBalanced,tx,w_logistic))\n",
    "\n",
    "print(logistic_loss(yBalanced,tx,initial_w))\n",
    "print(logistic_loss(yBalanced,tx,w_logistic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10105.0\n"
     ]
    }
   ],
   "source": [
    "predictions = (np.sign(logistic(tx@w_logistic)-0.5)+1)/2\n",
    "errors = np.abs(predictions - yBalanced)\n",
    "print(np.sum(errors))\n",
    "#for e in errors:\n",
    "#    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StdVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
