{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing numpy and functions\n",
    "import numpy as np\n",
    "import helpers as h\n",
    "from implementations import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded, there are 321 features and 328135 samples, the shapes of the unindexed data is:\n",
      "y: (328135, 1), x: (328135, 321)\n",
      "\n",
      "For a threshold of 0.7, there are 144 good features, and 177 bad features\n",
      "There remains in the data 289870 samples with at most 5 missing values\n",
      "The number of invalid entries remaing in the dataset is 272938\n",
      "That is 0.006538802834987332 parts of the whole dataset\n",
      "Removed 21050 samples with outliers more than 10 standard deviations from the mean. There remains 268820 samples in the dataset.\n",
      "Standardized data by subtracting the mean and dividing by the standard deviation\n",
      "\n",
      "Created a balanced subset of the data, with 46448 samples, 23224 each of positive and negative samples\n",
      "\n",
      "Added dummy variable and replaced invalid entries with zeros\n",
      "The resultant dataarray tx has shape (46448, 145)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Loading the data\n",
    "X, xHeader, Y, yHeader, indexedX, indexedXheader, indexedY, indexedYheader = loadTrainingData()\n",
    "print('')\n",
    "# Cleaning/feature engineering the data\n",
    "yClean, xClean, xHeaderClean, removedFeatures = dataCleaning(Y,X,xHeader)\n",
    "print('')\n",
    "# Making a balanced data set to force the model to not just predict negatively all the time\n",
    "yBalanced, xBalanced, balancePrior = balanceData(yClean,xClean)\n",
    "print('')\n",
    "# Adding dummy variables and replacing the remaining invalid values by the mean\n",
    "tx = makeTrainingData(xBalanced)\n",
    "print(f'The resultant dataarray tx has shape {tx.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Implementations of the functions to be used in the project.\n",
    "\n",
    "The six required functions, with their descriptions from the project description, are described below.\n",
    "Any helper functions are sorted with the first function they are used for.\n",
    "\n",
    "'''\n",
    "######### Importing Numpy ##########\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "########## Linear regression using gradient descent ##########\n",
    "\n",
    "def MSE(e):\n",
    "    '''\n",
    "    Calculates the mean squared error of the submitted error-array\n",
    "    Args:\n",
    "        e: (N,) array of the error fr all N predictions\n",
    "    Returns:\n",
    "        Float value of the mean squared error'''\n",
    "    return e.T @ e / len(e)\n",
    "\n",
    "\"\"\"\n",
    "def MAE(e): # Not used for now at least\n",
    "    '''\n",
    "    Calculates the mean absolute error of the submitted error-array\n",
    "    Args:\n",
    "        e: (N,) array of the error fr all N predictions\n",
    "    Returns:\n",
    "        Float value of the mean absolute error'''\n",
    "    return np.sum(np.abs(e)) / len(e)\n",
    "\"\"\"\n",
    "    \n",
    "def compute_loss(y, tx, w, lossFunction=MSE):\n",
    "    \"\"\"Calculate the loss using either MSE or MAE.\n",
    "    Args:\n",
    "        y: (N,) array with the labels\n",
    "        tx: (N,d) array with the samples and their features\n",
    "        w: (d,) array with the model parameters\n",
    "        lossFunction: The loss function of your choice. Defaults to mean square error\n",
    "    Returns:\n",
    "        Float value of the loss, given the selected loss-function\n",
    "    \"\"\"\n",
    "    return lossFunction(y - tx @ w)\n",
    "\n",
    "def compute_mse_gradient(y, tx, w):\n",
    "    \"\"\"Computes the gradient at w for the mean square error .\n",
    "    Args:\n",
    "        y: (N,) array with the labels\n",
    "        tx: (N,d) array with the samples and their features\n",
    "        w: (d,) array of model parameters/weights\n",
    "    Returns:\n",
    "        (d,) array containing the gradient at w\n",
    "    \"\"\"\n",
    "    return - tx.T @ (y-tx @ w) / len(y)\n",
    "\n",
    "def mean_squared_error_gd(y, tx, initial_w, max_iters, gamma): # Required function #1\n",
    "    \"\"\"The Gradient Descent (GD) algorithm for the mean square error\n",
    "    Args:\n",
    "        y: (N,) array with the labels\n",
    "        tx: (N,d) array with the samples and their features\n",
    "        initial_w: (d,) array with the initialization for the model parameters\n",
    "        max_iters: Integer denoting the maximum number of iterations of GD\n",
    "        gamma: Float denoting the stepsize\n",
    "    Returns:\n",
    "        w: (d,) array with the final parameters\n",
    "        loss: Float denoting the final loss\n",
    "    \"\"\"\n",
    "    # Initializing w\n",
    "    w = np.array(initial_w,dtype=float) # Making sure the array is a float array no matter the provided initialization\n",
    "    \n",
    "    for n in range(max_iters):\n",
    "        grad = compute_mse_gradient(y,tx,w) * gamma # Updating w by a step in the negative gradient direction at the current w\n",
    "        w -= grad\n",
    "        #print(f'w: {w}, grad: {grad}')\n",
    "    print(w.shape,y.shape,tx.shape)\n",
    "    return w, compute_loss(y,tx,w) # Returning the final loss and the final parameters\n",
    "\n",
    "def mse_gd_momentum(y, tx, initial_w, max_iters, gamma, beta=0.5):\n",
    "    \"\"\"The Gradient Descent (GD) algorithm for the mean square error, with momentum\n",
    "    Args:\n",
    "        y: (N,) array with the labels\n",
    "        tx: (N,d) array with the samples and their features\n",
    "        initial_w: (d,) array with the initialization for the model parameters\n",
    "        max_iters: Integer denoting the maximum number of iterations of GD\n",
    "        gamma: Float denoting the stepsize\n",
    "        beta: Float denoting the ratio between the former momentum and the current gradient\n",
    "    Returns:\n",
    "        w: (d,) array with the final parameters\n",
    "        loss: Float denoting the final loss\n",
    "    \"\"\"\n",
    "    # Initializing w\n",
    "    w = np.array(initial_w,dtype=float) # Making sure the array is a float array no matter the provided initialization\n",
    "    m = np.zeros(len(initial_w), dtype=float) # Initializing the momentum as the zero vector\n",
    "\n",
    "    for n in range(max_iters):\n",
    "        m = beta * m + (1-beta) * compute_mse_gradient(y,tx,w) # Weighted average of the former momentum and current gradient\n",
    "        w -= m * gamma # Updating w by a step in the negative momentum direction\n",
    "\n",
    "    return w, compute_loss(y,tx,w) # Returning the final loss and the final parameters\n",
    "\n",
    "\n",
    "\n",
    "########## Linear regression using stochastic gradient descent ##########\n",
    "\n",
    "'''\n",
    "def compute_stoch_gradient(y,tx, w):\n",
    "    \"\"\"Compute a stochastic gradient at w from a data sample batch of size B, where B < N, and their corresponding labels.\n",
    "    Args:\n",
    "        y: (B,) array of labels\n",
    "        tx: (B,d) array of samples and their features\n",
    "        w: (d,) array of model parameters\n",
    "    Returns:\n",
    "        (d,) array containing a stochastic gradient at w\n",
    "    \"\"\"\n",
    "    return - tx.T @ (y - tx @ w) / len(y)\n",
    "'''\n",
    "\n",
    "def mini_batch(y,tx,B):\n",
    "    '''Extract B random labels and their corresponding samples\n",
    "    Args: \n",
    "        y: (N,) array of labels\n",
    "        tx: (N,d) array of samples and their features\n",
    "        B: Integer denoting the desired batch size\n",
    "    Returns:\n",
    "        yBatch: (B,) array of the randomly extracted labels\n",
    "        txBatch: (B,d) array of the randomly extracted samples and their features\n",
    "    '''\n",
    "    shuffledIndexes = np.random.permutation(len(y)) # Produces an array of lenght N with the indices 0 to N-1 in a random permutation\n",
    "    yBatch, txBatch = y[shuffledIndexes[0:B]], tx[shuffledIndexes[0:B]] # Extracts the samples of y and tx corresponding to the first B indices in our randomly permuted index array\n",
    "    return yBatch, txBatch\n",
    "\n",
    "def mean_squared_error_sgd(y, tx, initial_w, max_iters, gamma, batch_size=1): # Required function #2\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD) for the mean square error\n",
    "    Args:\n",
    "        y: (N,) array with the labels\n",
    "        tx: (N,d) with the samples and their features\n",
    "        initial_w: (d,) array with the initialization for the model parameters\n",
    "        batch_size: Integer denoting the desired number of data points to use for computing the stochastic gradient\n",
    "        max_iters: Integer denoting the maximum number of iterations of SGD\n",
    "        gamma: Float denoting the stepsize\n",
    "    Returns:\n",
    "        w: (d,) array with the final parameters\n",
    "        loss: Float denoting the final loss\n",
    "    \"\"\"\n",
    "    # Initializing w\n",
    "    w = np.array(initial_w,dtype=float) # Making sure the array is a float array no matter the provided initialization\n",
    "\n",
    "    for n in range(max_iters):\n",
    "        batch_y, batch_x = mini_batch(y,tx,batch_size) # Extracting a batch of x's and corresponding y's to \n",
    "        w -= gamma * compute_mse_gradient(batch_y,batch_x,w) # Updating w by a step in the negative stochastic gradient descent direction\n",
    "        \n",
    "    return w, compute_loss(y,tx,w) # Returning the final loss and the final parameters\n",
    "\n",
    "def mse_sgd_momentum(y, tx, initial_w, max_iters, gamma, batch_size=1, beta=0.5):\n",
    "    \"\"\"The Stochastic Gradient Descent algorithm (SGD) for the mean square error, but with momentum\n",
    "    Args:\n",
    "        y: (N,) array with the labels\n",
    "        tx: (N,d) with the samples and their features\n",
    "        initial_w: (d,) array with the initialization for the model parameters\n",
    "        max_iters: Integer denoting the maximum number of iterations of SGD\n",
    "        gamma: Float denoting the stepsize\n",
    "        batch_size: Integer denoting the desired number of data points to use for computing the stochastic gradient\n",
    "        beta: float between 0 and 1 denoting the ratio between the last and the next step, i.e. a momentum parameter\n",
    "    Returns:\n",
    "        w: (d,) array with the final parameters\n",
    "        loss: Float denoting the final loss\n",
    "    \"\"\"\n",
    "    # Initializing w\n",
    "    w = np.array(initial_w,dtype=float) # Making sure the array is a float array no matter the provided initialization\n",
    "    m = np.zeros(len(initial_w),dtype=float) # Initializing the momentum as zero\n",
    "\n",
    "    for n in range(max_iters):\n",
    "        batch_y, batch_x = mini_batch(y,tx,batch_size) # Extracting a batch of x's and corresponding y's\n",
    "        m = beta * m + (1-beta) * compute_mse_gradient(batch_y,batch_x,w) # Updating the momentum by a linear combination of the current gradient and the former step\n",
    "        w -= gamma * m # Updating w by a step in the negative momentum direction\n",
    "        #print(f'{n+1}/{max_iters}: w: {w}')\n",
    "        \n",
    "    return w, compute_loss(y,tx,w) # Returning the final loss and the final parameters\n",
    "\n",
    "########## Least squares regression using normal equations ##########\n",
    "\n",
    "def least_squares(y,tx): # Required function #3\n",
    "    '''Computes the weights to minimize the mean square error, by way of the normal equations\n",
    "    Args:\n",
    "        tx: (N,d) array with the samples and their features\n",
    "        y: (N,) array with the labels\n",
    "    Returns:\n",
    "        w: (d,) array with the optimal least squares parameters\n",
    "        loss: Float denoting the least squares loss of the solution w\n",
    "    '''\n",
    "    w = np.linalg.solve(tx.T@tx,tx.T@y)\n",
    "    return w, compute_loss(y,tx,w)\n",
    "\n",
    "\n",
    "\n",
    "########## Ridge regression using normal equations ##########\n",
    "\n",
    "def ridge_regression(y, tx, lambda_): # Required function #4\n",
    "    ''' Implement ridge regression for the normal equations\n",
    "    Args:\n",
    "        y: (N,) array with the labels\n",
    "        tx: (N,d) array with the samples and their features\n",
    "        lambda_: Float denoting how much of a 'punisment' should be given for complex solutions\n",
    "    Returns:\n",
    "        w: (d,) array of the optimal parameters\n",
    "    '''\n",
    "\n",
    "    \"\"\"\n",
    "    It can be shown that for the ridge estimator \n",
    "    beta^hat(lambda) = argmin_beta 1/n ||y-tx@beta||**2 + lambda*beta.T @ beta\n",
    "    the ridge regression solution exists (even when X does not have full rank), and is given by\n",
    "    (tx.T @ tx + n*lambda*I) @ beta^hat(lambda) = tx.T @ y\n",
    "    where n =len(y), and I is the identity matrix\n",
    "    \"\"\"\n",
    "    w = np.linalg.solve(2 * len(y) * lambda_ * np.identity(tx.shape[1]) + tx.T@tx , tx.T@y)  # Computing w by way of the normal equations\n",
    "    return w, compute_loss(y,tx,w)\n",
    "\n",
    "\n",
    "\n",
    "########## Logistic regression using gradient descent or SGD (y ∈ {0,1}) ##########\n",
    "\n",
    "def logistic(z):\n",
    "    \"\"\" Applies the logistic (also called sigmoid) function on z\n",
    "    Args:\n",
    "        z: (N,) array\n",
    "    Returns:\n",
    "        (n,) array\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def logistic_loss(y, tx, w):\n",
    "    ''' Compute the cost by negative log likelihood.\n",
    "    Args:\n",
    "        y: (N,) array with labels\n",
    "        tx: (N,D) array with samples and their features\n",
    "        w: (D,) array with the parameters\n",
    "    Returns:\n",
    "        float non-negative loss\n",
    "    '''\n",
    "    #return np.sum(np.log( 1 + np.exp(-y * (tx@w))))\n",
    "    first_term = y.T@np.log(logistic(tx@w))\n",
    "    log_term = (1-y).T@np.log(1-logistic(tx@w))\n",
    "    return -np.sum(first_term+log_term) / y.shape[0]\n",
    "    #return - np.sum( y * np.log(logistic(tx @ w)) + (1-y)* np.log(1-logistic(tx @ w)) ) / y.shape[0]\n",
    "\n",
    "def logistic_gradient(y, tx, w):\n",
    "    ''' Compute the gradient of the logistic loss\n",
    "    Args:\n",
    "        y: (N,) array with labels\n",
    "        tx: (N,D) array with samples and their features\n",
    "        w: (D,) array with the parameters\n",
    "    Returns:\n",
    "        (D, 1) array with the gradient of the logistic loss with respect to the parameters\n",
    "    '''\n",
    "    return tx.T @ (logistic(tx@w)-y) / y.shape[0]\n",
    "\n",
    "def logistic_regression(y, tx, initial_w, max_iters, gamma):\n",
    "    ''' Use logistic regression for binary clasification\n",
    "    Args:\n",
    "        y: (N,) array with labels\n",
    "        tx: (N,D) array with samples and their features\n",
    "        initial_w: (D,) array with some initial parameters\n",
    "        max_iters: integer of the maximum number of iterations\n",
    "        gamma: float of the step length\n",
    "    Return:\n",
    "        w: The final parameters\n",
    "        loss: The final loss\n",
    "    '''\n",
    "    w = np.array(initial_w, dtype=float)\n",
    "    for i in range(max_iters):\n",
    "        w -= gamma * logistic_gradient(y,tx,w)\n",
    "    \n",
    "    return w, logistic_loss(y,tx,w)\n",
    "\n",
    "########## Regularized logistic regression using gradient descent or SGD (y ∈ {0,1}, with regularization term λ∥w∥2) ##########\n",
    "\n",
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss and gradient.\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1)\n",
    "        lambda_: scalar\n",
    "    Returns:\n",
    "        loss: scalar number\n",
    "        gradient: shape=(D, 1)\n",
    "    \"\"\"\n",
    "    grad = logistic_gradient(y,tx,w)\n",
    "    loss = logistic_loss(y,tx,w)\n",
    "    \n",
    "    loss_pen = loss+lambda_*np.sum(w.T@w)\n",
    "    grad_pen = grad+lambda_*np.abs(w)*2\n",
    "        \n",
    "    return loss_pen,grad_pen\n",
    "\n",
    "def reg_logistic_regression(y, tx, lambda_ ,initial_w, max_iters, gamma): # Required function #6\n",
    "    ''' Perform regularized logistic regression for binary classification\n",
    "    Args:\n",
    "        y: (N,) array with labels\n",
    "        tx: (N,D) array with samples and their features\n",
    "        lambda_: float of the penalization parameter\n",
    "        initial_w: (D,) array with some initial parameters\n",
    "        max_iters: integer of the maximum number of iterations\n",
    "        gamma: float of the step length\n",
    "    Returns:\n",
    "        w: The final parameters\n",
    "        loss: The final loss\n",
    "    '''\n",
    "    w = np.array(initial_w, dtype=float)\n",
    "\n",
    "    for n in range(max_iters):\n",
    "        #loss_pen, grad_pen = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "        w -= gamma * logistic_gradient(y,tx,w) + 2*lambda_*np.abs(w) # Updating the paramters by the gradient with the regularization term\n",
    "    \n",
    "    loss = logistic_loss(y,tx,w)\n",
    "    return w, loss\n",
    "\n",
    "def calculate_hessian(y, tx, w):\n",
    "    \"\"\" Return the Hessian of the loss function.\n",
    "    Args:\n",
    "        y:  (N, 1)\n",
    "        tx: (N, D)\n",
    "        w:  (D, 1)\n",
    "    Returns:\n",
    "        (D,D) array of the hessian matrix\n",
    "    logisticDiag = np.diag((logistic * (1-logistic)).flatten()) \n",
    "    \"\"\"\n",
    "    sig = logistic(tx@w)\n",
    "    \n",
    "    S = np.diag(sig-sig@sig.T)\n",
    "    \n",
    "    hess = (1/np.shape(y)[0])*(tx.T@np.diag(S)@tx)\n",
    "    \n",
    "    return hess\n",
    "\n",
    "\n",
    "########## Loading data ##########\n",
    "\n",
    "def loadData(dataPath):\n",
    "    ''' Loads data and returns it as masked numpy array. A masked array contains information about which values are invalid, ensuring methods like .mean() ignores the masked values\n",
    "    Args:\n",
    "        dataPath: The file path of the data\n",
    "    Returns:\n",
    "        data: (N,d) masked numpy array, where N is the number of samples, and d is the dimension of the x values, or 1 if the data in question are the labels\n",
    "        header: (d,) array with the column names\n",
    "    '''\n",
    "    data = np.genfromtxt(dataPath, delimiter=',', skip_header=1, dtype=float, usemask=True) # Loading the data as a masked array (with usemask=True), skipping the header, and specifying that the values are floats\n",
    "    header = np.genfromtxt(dataPath, delimiter=',', dtype=str, max_rows=1) # Loading the first row of the csv file, i.e. the header\n",
    "    return data , header\n",
    "\n",
    "def loadTrainingData():\n",
    "    ''' Loads the medical training data and nothing else. Wrapper function\n",
    "    Returns:\n",
    "        X, xHeader, Y, yHeader, indexedX, indexedXheader, indexedY, indexedYheader\n",
    "    '''\n",
    "    x, xHeader = loadData('./Data/x_train.csv')\n",
    "    y, yHeader = loadData('./Data/y_train.csv')\n",
    "    y[y == -1] = 0\n",
    "    unIndexedX, unIndexedXHeader = x[:,1:], xHeader[1:]\n",
    "    unIndexedY, unIndexedYHeader = y[:,1:], yHeader[1:]\n",
    "\n",
    "    print(f'Data successfully loaded, there are {unIndexedX.shape[1]} features and {y.shape[0]} samples, the shapes of the unindexed data is:\\ny: {unIndexedY.shape}, x: {unIndexedX.shape}')\n",
    "\n",
    "    return unIndexedX, unIndexedXHeader, unIndexedY.flatten(), unIndexedYHeader, x, xHeader, y, yHeader\n",
    "\n",
    "########## Data cleaning functions ##########\n",
    "\n",
    "def removeBadFeatures(x,xHeader,threshold=0.7):\n",
    "    ''' Function checking how many features have more than {threshold} parts valid entries, and removing the 'bad' features with less than {threshold} valid entries\n",
    "    Args:\n",
    "        x: (N,d) array of the dataset\n",
    "        xHeader: (d,) array of the header for the dataset\n",
    "        threshold: float between 0 and 1\n",
    "    Returns:\n",
    "        (N,d-b) array of the new dataset, where b is the number of features with less than threshold parts valid entries\n",
    "        (d-b,) array of the new header, where b is the number of features with less than threshold parts valid entries\n",
    "        (b,) array with the removed features\n",
    "    '''\n",
    "    # Counting the number of valid values for each feature, and calculating the percentage of valid entries\n",
    "    validFeatureVals = x.count(axis=0) # The number of valid entries for each feature\n",
    "    validFeatureValsPercent = validFeatureVals/x.shape[0] # The percentage of valid entries for each feature\n",
    "\n",
    "    # Finding the indices of all the features with number of features above and below a threeshold\n",
    "    featureIndicesAboveThreeshold = np.argwhere(validFeatureValsPercent > threshold).flatten() # Finding the indices where there are more than threeshold percent valid entries\n",
    "    featureIndicesBeneathThreeshold = np.argwhere(validFeatureValsPercent < threshold).flatten()\n",
    "\n",
    "    # Printing the good vs bad features\n",
    "    print(f'For a threshold of {threshold}, there are {len(featureIndicesAboveThreeshold)} good features, and {x.shape[1]-len(featureIndicesAboveThreeshold)} bad features')\n",
    "\n",
    "    # Removing the features that appears less than {threeshold} of the time, and returning the others\n",
    "    return x[:,featureIndicesAboveThreeshold], xHeader[featureIndicesAboveThreeshold], xHeader[featureIndicesBeneathThreeshold]\n",
    "\n",
    "\n",
    "def removeBadSamples(y,x,acceptableMissingValues):\n",
    "    ''' Function checking how many samples miss more than {acceptableMissingValues} values, and removing those samples\n",
    "    Args:\n",
    "        y: (N,) array of the labels\n",
    "        x: (N,d) array of the data\n",
    "        acceptableMissingValues: integer between 0 and d\n",
    "    Returns:\n",
    "        (N-b) array of the new labels, where b is the number of samples missing more than {acceptableMissingValues}\n",
    "        (N-b,d) array of the new dataset, where b is the number of samples missing more than {acceptableMissingValues}\n",
    "    '''\n",
    "    # Counting the number of remaining valid entries for each sample\n",
    "    validSampleVals = x.count(axis=1)\n",
    "\n",
    "    # Find the indices of the samples with more than {acceptableMissingValues} invalid missing\n",
    "    sampleIndicesAboveThreeshold = np.argwhere(validSampleVals >= x.shape[1]-acceptableMissingValues).flatten()\n",
    "    print(f'There remains in the data {len(sampleIndicesAboveThreeshold)} samples with at most {acceptableMissingValues} missing values')\n",
    "\n",
    "    # Removing samples with more than {acceptableMissingValues} missing values\n",
    "    return y[sampleIndicesAboveThreeshold], x[sampleIndicesAboveThreeshold]\n",
    "\n",
    "\n",
    "def standardizeData(x):\n",
    "    ''' Function for standardizing the data\n",
    "    Args:\n",
    "        x: (N,d) array\n",
    "    Returns:\n",
    "        (N,d) array where x has been subtracted its mean, and divided by its standard deviation\n",
    "    '''\n",
    "    return (x - np.mean(x, axis=0)) / np.std(x, axis=0) # Subtract the mean and divide by the standard deviation\n",
    "\n",
    "def balanceData(y,x):\n",
    "    ''' Function for balancing the number of positive and negative cases in the dataset for regression, which may help find real correlations instead of just guessing based on the prior\n",
    "    Args:\n",
    "        y: (N,) array with labels\n",
    "        x: (N,d) array with data\n",
    "    Returns:\n",
    "        (N_b,) array with labels, where N_b is min(positiveCase, negativeCases) *2\n",
    "        (N_b,d) array with the balanced data\n",
    "    '''\n",
    "    # Extracting the indices of the positive and negative cases, bundling them in a tuple, and bundling their lengths in tuples\n",
    "    positiveCases = np.where(y == 1)[0]\n",
    "    negativeCases = np.where(y == 0)[0]\n",
    "    casesIndices = (positiveCases,negativeCases)\n",
    "    casesLengths = (len(positiveCases),len(negativeCases))\n",
    "\n",
    "    # Finding which subset is the smallest and largest (aka are there more negative or positive cases), and setting the smallestSubsetLength to the length of the smallest subset\n",
    "    smallestSubset = np.argmin(casesLengths)\n",
    "    largestSubset = np.argmax(casesLengths)\n",
    "    smallestSubsetLength = casesLengths[smallestSubset]\n",
    "\n",
    "    # Storing the cases from the smallest subset in an array\n",
    "    balancedY = np.zeros(smallestSubsetLength*2)\n",
    "    balancedX = np.zeros((smallestSubsetLength*2,x.shape[1]))\n",
    "    balancedY[:smallestSubsetLength] = (y[casesIndices[smallestSubset]]).flatten()\n",
    "    balancedX[:smallestSubsetLength] = x[casesIndices[smallestSubset]]\n",
    "\n",
    "    # Randomly choosing as many samples from the largest subset as there are in the smallest subset, and storing them in the balanced array\n",
    "    randomSampleIndices = np.random.permutation(casesLengths[largestSubset])[:smallestSubsetLength]\n",
    "    balancedX[smallestSubsetLength:] = (x[casesIndices[largestSubset]])[randomSampleIndices]\n",
    "    balancedY[smallestSubsetLength:] = ((y[casesIndices[largestSubset]])[randomSampleIndices]).flatten()\n",
    "\n",
    "    # Shuffling the balanced arrays so no model can learn to classify the entries by their position in the dataset\n",
    "    shufflingIndices = np.random.permutation(balancedY.shape[0])\n",
    "    shuffledBalancedX = balancedX[shufflingIndices]\n",
    "    shuffledBalancedY = balancedY[shufflingIndices]\n",
    "\n",
    "    print(f'Created a balanced subset of the data, with {2*smallestSubsetLength} samples, {smallestSubsetLength} each of positive and negative samples')\n",
    "    \n",
    "    ### Models trained from this dataset will overestimate the probability of positive (or negative) cases,\n",
    "    ### therefore we should use some bayesian probabilities to update probabilities\n",
    "    prior = 2*smallestSubsetLength/len(y) # The probability of a random sample being in the balanced data\n",
    "    return shuffledBalancedY, shuffledBalancedX, prior\n",
    "\n",
    "def makeTrainingData(x):\n",
    "    ''' Function filling the invalid values with the mean (zero), and adding a dummy variable'''\n",
    "    xClean = np.ma.filled(x,fill_value=0) # Replace the invalid entries by zeros (aka the mean)\n",
    "    tx = np.c_[np.ones(xClean.shape[0]),xClean] # Adding a dummy feature\n",
    "    print('Added dummy variable and replaced invalid entries with zeros')\n",
    "    return np.nan_to_num(tx) # For some reason, not all NaN values were filled with zeros, this should rectify that problem\n",
    "\n",
    "def detectOutliers(y, x, outlierThreshold=10):\n",
    "    xStandardized = standardizeData(x)\n",
    "\n",
    "    stdAwayFromMean = np.abs(xStandardized)\n",
    "\n",
    "    # Assuming normal distribution, approx 68% of the data falls within 1 std, 95% within 2 std, 99.7% within 3 std\n",
    "    # Therefore one may consider any entries in stdAwayFromMean > {outlierThreeshold=3} to be outliers - except that the data is not neccesarily normally distributed, so the threshold should be higher\n",
    "    inlierRows = np.where(np.all(stdAwayFromMean < outlierThreshold, axis=1))[0]\n",
    "    \n",
    "    outlierRows = np.where(np.any(stdAwayFromMean > outlierThreshold, axis=1))[0]\n",
    "    #print(outlierRows.shape)\n",
    "    #print(inlierRows.shape)\n",
    "    #print(x.shape)\n",
    "\n",
    "    print(f'Removed {len(outlierRows)} samples with outliers more than {outlierThreshold} standard deviations from the mean. There remains {len(inlierRows)} samples in the dataset.')\n",
    "\n",
    "    return y[inlierRows], x[inlierRows]\n",
    "\n",
    "def dataCleaning(y,x,xHeader,featureThreshold=0.7,acceptableMissingValues=5):\n",
    "    ''' Function for removing features with to few valid entries, samples with to few valid entries, samples with outliers, and standardizing the data.\n",
    "    Args: \n",
    "        y: (N,) array of the labels\n",
    "        x: (N,d) array of the samples and their features\n",
    "        xHeader: (d,) array of the feature titles\n",
    "        featureThreshold: float between 0 and 1 of the percent of a features entries must be valid to keep the feature\n",
    "        acceptableMissingValues: integer of the number of entries for each feature that may be invalid entries\n",
    "    Returns:\n",
    "        (N-c) array of labels, where c is the number of samples with too many missing entries, plus the number of samples with outliers\n",
    "        (N-c,d-b) array of the data, where b is the number of removed features\n",
    "        (d-b,) array of the feature titles\n",
    "        (b,) array of the removed feature titles\n",
    "    '''\n",
    "    # Removing bad features and samples\n",
    "    xFeaturesRemoved, xHeaderFeaturesRemoved, removedFeatures = removeBadFeatures(x,xHeader,featureThreshold)\n",
    "    ySamplesRemoved, xSamplesRemoved = removeBadSamples(y,xFeaturesRemoved,acceptableMissingValues)\n",
    "    print(f'The number of invalid entries remaing in the dataset is {xSamplesRemoved.size - xSamplesRemoved.count()}\\nThat is {(xSamplesRemoved.size - xSamplesRemoved.count())/xSamplesRemoved.size} parts of the whole dataset')\n",
    "    \n",
    "    # Removing outliers\n",
    "    yOutliersRemoved, xOutliersRemoved = detectOutliers(ySamplesRemoved,xSamplesRemoved)\n",
    "\n",
    "    # Standardizing the data by subtraction of the mean and dividing by the standard deviation\n",
    "    xStandardized = standardizeData(xOutliersRemoved)\n",
    "    print('Standardized data by subtracting the mean and dividing by the standard deviation')\n",
    "\n",
    "    return yOutliersRemoved, xStandardized, xHeaderFeaturesRemoved, removedFeatures\n",
    "\n",
    "\n",
    "\n",
    "########## K Fold Cross Validation #########\n",
    "\n",
    "def k_fold_cross_validation_sets(y,x,K):\n",
    "    ''' Function for making K separate training sets out of the provided dataset\n",
    "    Args:\n",
    "        y: (N,) array of the labels\n",
    "        x: (N,d) array of the data with its features\n",
    "        K: Integer number of separate trainingsets\n",
    "    Yields:\n",
    "        y_k: (N/K,) array of the chosen labels. N/K is N//K + 1 for the first sets, and N//K for the rest of the sets\n",
    "        x_k: (N/K,d) array of the data\n",
    "    '''\n",
    "    N = len(y)      # Saving the number of samples as an integer\n",
    "    batchSize = N // K  # Calculating the batch size\n",
    "    residual = N - K*batchSize  # Checking how many samples would not be included in sets of size N//K\n",
    "\n",
    "    indices = np.random.permutation(N) # Randomly permuted indices of the provided dataset\n",
    "    \n",
    "    for k in range(K):\n",
    "        if k < residual: # If the samples 'in' the residual has not 'been used', we include them\n",
    "            indices_k = indices[k*(batchSize+1):(k+1)*(batchSize+1)] # Indices of the elements for each k batch. Here included one extra samples 'from' the residual\n",
    "        else:\n",
    "            indices_k = indices[residual+k*batchSize:residual+(k+1)*batchSize] # Indices of the elements for each k batch\n",
    "        \n",
    "        yield y[indices_k], x[indices_k] # Yield returns the first set, and next time the function is called the code continues, so the for loop repeats and yields the next set\n",
    "\n",
    "def k_fold_cross_validation(y,tx,K,initial_w,max_iters,gamma,lambda_,batch_size, regressionFunction=logistic_regression, lossFunction=logistic_loss):\n",
    "    ''' Performing regression on K separate subsets of the provided training set, and returning the average parameters\n",
    "    Args:\n",
    "        y: (N,) array of the labels\n",
    "        tx: (N,d) array of the data and its features\n",
    "        initital_w: (d,) array with some initialization of the parameters\n",
    "        max_iters: integer of the maximum iterations per regression\n",
    "        gamma: float of the step size\n",
    "        regressionFunction: The function of the chosen type of regression\n",
    "        lossFunction: The function of the chosen type of loss\n",
    "    Returns:\n",
    "        w_avg: (d,) array of the resultant parameters averaged over the cross validation runs\n",
    "    ''' \n",
    "    crossValidationSets = k_fold_cross_validation_sets(y,tx,K)\n",
    "    \n",
    "    w, loss = np.zeros((K,tx.shape[1])), np.zeros(K)\n",
    "    \n",
    "    for k in range(K):\n",
    "        y_k, tx_k = next(crossValidationSets)\n",
    "        if regressionFunction == ridge_regression:\n",
    "            w[k], loss[k] = regressionFunction(y_k, tx_k, gamma)\n",
    "        elif regressionFunction == reg_logistic_regression:\n",
    "            w[k], loss[k] = regressionFunction(y_k, tx_k,lambda_, initial_w, max_iters, gamma)\n",
    "\n",
    "        elif regressionFunction == mse_sgd_momentum or regressionFunction == mean_squared_error_sgd:\n",
    "            \n",
    "            w[k], loss[k] = regressionFunction(y_k, tx_k, initial_w, max_iters, gamma, batch_size=batch_size)\n",
    "            \n",
    "        elif regressionFunction == mse_gd_momentum or regressionFunction == mean_squared_error_gd:\n",
    "            #(y, tx, initial_w, max_iters, gamma)\n",
    "            w[k], loss[k] = regressionFunction(y_k, tx_k, initial_w, max_iters, gamma)\n",
    "            \n",
    "        else:\n",
    "            w[k], loss[k] = regressionFunction(y_k, tx_k, initial_w, max_iters, gamma) #(y, tx, lambda_ ,initial_w, max_iters, gamma)\n",
    "\n",
    "        print(f'Run {k+1} yielded a loss improvement from {lossFunction(y_k,tx_k,initial_w)} to {lossFunction(y_k,tx_k,w[k])}')\n",
    "    w_avg = np.sum(w,axis=0) / K\n",
    "    \n",
    "    print(f'''-----------------------------------------------------------------------------------------\n",
    "Averaging the parameters, the loss improves from {lossFunction(y,tx,initial_w)} to {lossFunction(y,tx,w_avg)}''')\n",
    "    return w_avg, lossFunction(y_k,tx_k,initial_w)\n",
    "\n",
    "\n",
    "########## Making final predictions ##########\n",
    "\n",
    "def makePredictions(w,xTest,xHeader,xHeaderFeaturesRemoved, prior=1.0):\n",
    "    ''' Function making predictions based on provided parameters and data\n",
    "    Args:\n",
    "        w: (d,) array with the parameters\n",
    "        x: (N,D) array with the data\n",
    "        xHeader: (D,) array with all the features\n",
    "        xHeader: (d,) array with the features that are actually used\n",
    "        prior: float denoting the probability of a random sample being in the model training data\n",
    "    Returns:\n",
    "        (N,) boolean array of the predictions\n",
    "    '''\n",
    "    standardX = standardizeData(xTest)\n",
    "    removedFeaturesX = standardX[:,np.nonzero(np.isin(xHeader, xHeaderFeaturesRemoved))[0]]\n",
    "    predictionSet = makeTrainingData(removedFeaturesX)\n",
    "    probabilities = prior * logistic(predictionSet@w) # The prob of the model being applicable times the prob from the model\n",
    "    return (np.sign(probabilities-0.5)+1)/2 # Shifting the probs to be negative for negative preds, and vice versa, taking the sign, shifting the preds up to be zero or two, diving by to so the preds are zero or one\n",
    "\n",
    "\n",
    "######## Calculating the recall of our prediction #####################\n",
    "\n",
    "def calculate_recall(y_true, y_predicted):\n",
    "    ''' Function that calculates the recall of our prediction\n",
    "    Args:\n",
    "        y_true: (N,2) array with the actuall data\n",
    "        y_predicted: (N,2) array with the predicted y's\n",
    "    Returns:\n",
    "        a scalar that is the recall\n",
    "    '''\n",
    "    true_positives = 0\n",
    "    false_negatives = 0\n",
    "    for i in range(len(y_true)):\n",
    "        true_label = y_true[i, 1]\n",
    "        pred_label = y_predicted[i, 1]\n",
    "        if true_label == 1 and pred_label == 1:\n",
    "            true_positives += 1\n",
    "        elif true_label == 1 and pred_label == 0:\n",
    "            false_negatives += 1\n",
    "    return true_positives / (true_positives + false_negatives)\n",
    "def determineLambda(y,tx,initial_w,lambdas):\n",
    "    w_reg_logistic = np.zeros((len(lambdas),len(initial_w)),dtype=float)\n",
    "    test_loss, train_loss = np.zeros(len(lambdas)), np.zeros(len(lambdas))\n",
    "    for i,l in enumerate(lambdas):\n",
    "        reg_logistic_regression_fixed_lambda = lambda y, tx, initial_w, max_iters, gamma: reg_logistic_regression(y,tx,l,initial_w,max_iters,gamma)\n",
    "    \n",
    "        w_reg_logistic[i], train_loss[i], test_loss[i] = k_fold_cross_validation(y,tx,K,initial_w,max_iter,gamma,reg_logistic_regression_fixed_lambda)\n",
    "    bestLambdaIndex = np.argmin(test_loss)\n",
    "    return train_loss, test_loss, lambdas[bestLambdaIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training a model with gradient descent\n",
      "(145,) (46448,) (46448, 145)\n",
      "training a model with gradient descent with momentum\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "compute_loss() takes from 3 to 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/maxgrobbelaar/Python/ml-project-1-andermaxjohannes/MachineLearning.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxgrobbelaar/Python/ml-project-1-andermaxjohannes/MachineLearning.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m#w_gd, loss_gd = k_fold_cross_validation(yBalanced,tx,K,initial_w,max_iter,gamma,batch_size, mean_squared_error_gd, compute_loss)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxgrobbelaar/Python/ml-project-1-andermaxjohannes/MachineLearning.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxgrobbelaar/Python/ml-project-1-andermaxjohannes/MachineLearning.ipynb#W4sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Training a model with gradient descent with momentum\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxgrobbelaar/Python/ml-project-1-andermaxjohannes/MachineLearning.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtraining a model with gradient descent with momentum\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/maxgrobbelaar/Python/ml-project-1-andermaxjohannes/MachineLearning.ipynb#W4sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m w_gd_m, loss_gd_m \u001b[39m=\u001b[39m k_fold_cross_validation(yBalanced,tx,K,initial_w,max_iter,gamma,batch_size, mse_gd_momentum, compute_loss)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxgrobbelaar/Python/ml-project-1-andermaxjohannes/MachineLearning.ipynb#W4sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxgrobbelaar/Python/ml-project-1-andermaxjohannes/MachineLearning.ipynb#W4sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Training a model with stochastic gradient descent\u001b[39;00m\n",
      "\u001b[1;32m/Users/maxgrobbelaar/Python/ml-project-1-andermaxjohannes/MachineLearning.ipynb Cell 4\u001b[0m line \u001b[0;36m5\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/maxgrobbelaar/Python/ml-project-1-andermaxjohannes/MachineLearning.ipynb#W4sZmlsZQ%3D%3D?line=578'>579</a>\u001b[0m         w[k], loss[k] \u001b[39m=\u001b[39m regressionFunction(y_k, tx_k, initial_w, max_iters, gamma)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/maxgrobbelaar/Python/ml-project-1-andermaxjohannes/MachineLearning.ipynb#W4sZmlsZQ%3D%3D?line=580'>581</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/maxgrobbelaar/Python/ml-project-1-andermaxjohannes/MachineLearning.ipynb#W4sZmlsZQ%3D%3D?line=581'>582</a>\u001b[0m         w[k], loss[k] \u001b[39m=\u001b[39m regressionFunction(y_k, tx_k, initial_w, max_iters, gamma) \u001b[39m#(y, tx, lambda_ ,initial_w, max_iters, gamma)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/maxgrobbelaar/Python/ml-project-1-andermaxjohannes/MachineLearning.ipynb#W4sZmlsZQ%3D%3D?line=583'>584</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mRun \u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m yielded a loss improvement from \u001b[39m\u001b[39m{\u001b[39;00mlossFunction(y_k,tx_k,initial_w)\u001b[39m}\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m{\u001b[39;00mlossFunction(y_k,tx_k,w[k])\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/maxgrobbelaar/Python/ml-project-1-andermaxjohannes/MachineLearning.ipynb#W4sZmlsZQ%3D%3D?line=584'>585</a>\u001b[0m w_avg \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(w,axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39m/\u001b[39m K\n",
      "\u001b[0;31mTypeError\u001b[0m: compute_loss() takes from 3 to 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "# Initializing the parameters at zero and setting some constants\n",
    "initial_w = np.zeros(tx.shape[1])\n",
    "K = 5\n",
    "gamma = 0.01\n",
    "max_iter = 1000\n",
    "batch_size = 1\n",
    "lambda_ = 0.1\n",
    "# Training a model with gradient descent\n",
    "print('Training a model with gradient descent')\n",
    "#(y, tx, initial_w, max_iters, gamma)\n",
    "w_gd, loss_gd = mean_squared_error_gd(yBalanced, tx, initial_w, max_iter, gamma)\n",
    "#w_gd, loss_gd = k_fold_cross_validation(yBalanced,tx,K,initial_w,max_iter,gamma,batch_size, mean_squared_error_gd, compute_loss)\n",
    "\n",
    "# Training a model with gradient descent with momentum\n",
    "print('training a model with gradient descent with momentum')\n",
    "w_gd_m, loss_gd_m = k_fold_cross_validation(yBalanced,tx,K,initial_w,max_iter,gamma,batch_size, mse_gd_momentum, compute_loss)\n",
    "print('')\n",
    "# Training a model with stochastic gradient descent\n",
    "print('Training a model with stochastic gradient descent')\n",
    "w_sgd, loss_sgd = k_fold_cross_validation(yBalanced,tx,K,initial_w,max_iter,gamma, batch_size, mean_squared_error_sgd, compute_loss)\n",
    "\n",
    "# Training a model with stochastic gradient descent with momentum\n",
    "print('Training a model with stochastic gradient descent with momentum')\n",
    "w_sgd_m, loss_sgd_m = k_fold_cross_validation(yBalanced,tx,K,initial_w,max_iter,gamma, batch_size, mse_sgd_momentum, compute_loss)\n",
    "\n",
    "# Training a model with least squares\n",
    "#w_lsqaures, loss_lsquare = least_squares(yBalanced,tx)#,K,initial_w,max_iter,gamma, least_squares, compute_loss)\n",
    "\n",
    "# Training a model with ridge regression\n",
    "print('Training a model with ridge regression')\n",
    "w_ridge, loss_ridge = k_fold_cross_validation(yBalanced,tx,K,initial_w,max_iter,gamma, batch_size, ridge_regression, compute_loss)\n",
    "\n",
    "# Training a model with logistic regression\n",
    "print('Training a model with logistic regression')\n",
    "w_logistic, loss_logistic = k_fold_cross_validation(yBalanced,tx,K,initial_w,max_iter,gamma, batch_size, logistic_regression, compute_loss)\n",
    "\n",
    "# Training a model with reg logistic regression and calculate best lambda\n",
    "print('Training a model with reg logistic regression')\n",
    "w_reg_logistic, loss_reg_logistic = k_fold_cross_validation(yBalanced,tx,K,initial_w,max_iter,gamma, batch_size, reg_logistic_regression, compute_loss)\n",
    "# lambdas = np.logspace(-2,0,20)\n",
    "# hwg_train, hwg_test, best_lambda, best_w = determineLambda(y_hwg.flatten(),tx_hwg,np.zeros(tx_hwg.shape[1]),lambdas,max_iter,K,gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(109379, 322)\n"
     ]
    }
   ],
   "source": [
    "# Loading the test data\n",
    "xTest, xIndexedHeader = loadData('./Data/x_test.csv')\n",
    "print(xTest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added dummy variable and replaced invalid entries with zeros\n",
      "Added dummy variable and replaced invalid entries with zeros\n",
      "106378.0 33139.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "def makePredictions(w,xTest,xHeader,xHeaderFeaturesRemoved, prior=1.0):\n",
    "    ''' Function making predictions based on provided parameters and data\n",
    "    Args:\n",
    "        w: (d,) array with the parameters\n",
    "        x: (N,D) array with the data\n",
    "        xHeader: (D,) array with all the features\n",
    "        xHeader: (d,) array with the features that are actually used\n",
    "        prior: float denoting the probability of a random sample being in the model training data\n",
    "    Returns:\n",
    "        (N,) boolean array of the predictions\n",
    "    '''\n",
    "    standardX = standardizeData(xTest)\n",
    "    removedFeaturesX = standardX[:,np.nonzero(np.isin(xHeader, xHeaderFeaturesRemoved))[0]]\n",
    "    predictionSet = makeTrainingData(removedFeaturesX)\n",
    "    probabilities = prior * logistic(predictionSet@w) # The prob of the model being applicable times the prob from the model\n",
    "    return (np.sign(probabilities-0.5)+1)/2 # Shifting the probs to be negative for negative preds, and vice versa, taking the sign, shifting the preds up to be zero or two, diving by to so the preds are zero or one\n",
    "\"\"\"\n",
    "# Making predictions\n",
    "pred_gd = makePredictions(w_gd_m,xTest[:,1:],xHeader,xHeaderClean)\n",
    "pred_logistic = makePredictions(w_logistic,xTest[:,1:],xHeader,xHeaderClean)\n",
    "pred_lsquares = makePredictions(w_lsquares,xTest[:,1:],xHeader,xHeaderClean)\n",
    "# Counting predicted positive cases\n",
    "print(np.sum(pred_gd),np.sum(pred_logistic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_logistic[pred_logistic == 0] = -1\n",
    "#h.create_csv_submission(xTest[:,0],pred_logistic,'./Predictions/balancedDataNoPrior.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "StdVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
