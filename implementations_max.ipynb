{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa400b74-6b8a-47a7-a139-c7f5996b2d54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(t):\n",
    "    \"\"\"apply sigmoid function on t.\n",
    "\n",
    "    Args:\n",
    "        t: scalar or numpy array\n",
    "\n",
    "    Returns:\n",
    "        scalar or numpy array\n",
    "\n",
    "    >>> sigmoid(np.array([0.1]))\n",
    "    array([0.52497919])\n",
    "    >>> sigmoid(np.array([0.1, 0.1]))\n",
    "    array([0.52497919, 0.52497919])\n",
    "    \"\"\"\n",
    "    phi = (1 + np.exp(-t))**(-1)\n",
    "    return phi\n",
    "    #raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "def calculate_loss(y, tx, w):\n",
    "    \"\"\"compute the cost by negative log likelihood.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1)\n",
    "\n",
    "    Returns:\n",
    "        a non-negative loss\n",
    "\n",
    "    >>> y = np.c_[[0., 1.]]\n",
    "    >>> tx = np.arange(4).reshape(2, 2)\n",
    "    >>> w = np.c_[[2., 3.]]\n",
    "    >>> round(calculate_loss(y, tx, w), 8)\n",
    "    1.52429481\n",
    "    \"\"\"\n",
    "    assert y.shape[0] == tx.shape[0]\n",
    "    assert tx.shape[1] == w.shape[0]\n",
    "    \n",
    "    #print(np.shape(tx),np.shape(y))\n",
    "    \n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    \n",
    "    first_term = y.T@np.log(sigmoid(tx@w))\n",
    "    log_term = (1-y).T@np.log(1-sigmoid(tx@w))\n",
    "    loss = -(1/np.shape(y)[0])*np.sum(first_term+log_term)\n",
    "    return loss\n",
    "    # ***************************************************\n",
    "    #raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_gradient(y, tx, w):\n",
    "    \"\"\"compute the gradient of loss.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1)\n",
    "\n",
    "    Returns:\n",
    "        a vector of shape (D, 1)\n",
    "\n",
    "    >>> np.set_printoptions(8)\n",
    "    >>> y = np.c_[[0., 1.]]\n",
    "    >>> tx = np.arange(6).reshape(2, 3)\n",
    "    >>> w = np.array([[0.1], [0.2], [0.3]])\n",
    "    >>> calculate_gradient(y, tx, w)\n",
    "    array([[-0.10370763],\n",
    "           [ 0.2067104 ],\n",
    "           [ 0.51712843]])\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    #grad=1/y.shape[0]*(-tx.T@y+tx.T@sigmoid(tx@w))\n",
    "                  \n",
    "    grad = (1/np.shape(y)[0])*tx.T@(sigmoid(tx@w)-y)\n",
    "    return grad\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError(\"Calculate gradient\")\n",
    "\n",
    "\n",
    "def learning_by_gradient_descent(y, tx, w, gamma):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent using logistic regression. Return the loss and the updated w.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1)\n",
    "        gamma: float\n",
    "\n",
    "    Returns:\n",
    "        loss: scalar number\n",
    "        w: shape=(D, 1)\n",
    "\n",
    "    >>> y = np.c_[[0., 1.]]\n",
    "    >>> tx = np.arange(6).reshape(2, 3)\n",
    "    >>> w = np.array([[0.1], [0.2], [0.3]])\n",
    "    >>> gamma = 0.1\n",
    "    >>> loss, w = learning_by_gradient_descent(y, tx, w, gamma)\n",
    "    >>> round(loss, 8)\n",
    "    0.62137268\n",
    "    >>> w\n",
    "    array([[0.11037076],\n",
    "           [0.17932896],\n",
    "           [0.24828716]])\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # TODO\n",
    "    grad = calculate_gradient(y, tx, w)\n",
    "    loss = calculate_loss(y,tx,w)\n",
    "    w_new = w-gamma*grad\n",
    "    \n",
    "    return loss,w_new\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "\n",
    "def calculate_hessian(y, tx, w):\n",
    "    \"\"\"return the Hessian of the loss function.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1)\n",
    "\n",
    "    Returns:\n",
    "        a hessian matrix of shape=(D, D)\n",
    "\n",
    "    >>> y = np.c_[[0., 1.]]\n",
    "    >>> tx = np.arange(6).reshape(2, 3)\n",
    "    >>> w = np.array([[0.1], [0.2], [0.3]])\n",
    "    >>> calculate_hessian(y, tx, w)\n",
    "    array([[0.28961235, 0.3861498 , 0.48268724],\n",
    "           [0.3861498 , 0.62182124, 0.85749269],\n",
    "           [0.48268724, 0.85749269, 1.23229813]])\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # calculate Hessian: TODO\n",
    "    #print('shape tx: ',np.shape(tx),'shape w: ',np.shape(w), np.shape(tx@w),np.shape(sigmoid(tx@w)))\n",
    "    sig = sigmoid(tx@w)\n",
    "    \n",
    "    S = np.diag(sig-sig@sig.T)\n",
    "    \n",
    "    print('tx ', np.shape(tx), 'diag ', np.shape((S)),'tx ', np.shape(tx))\n",
    "    hess = (1/np.shape(y)[0])*(tx@np.diag(S)@tx)\n",
    "    \n",
    "    return hess\n",
    "    # ***************************************************\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "def logistic_regression(y, tx, w):\n",
    "    \"\"\"return the loss, gradient of the loss, and hessian of the loss.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1)\n",
    "\n",
    "    Returns:\n",
    "        loss: scalar number\n",
    "        gradient: shape=(D, 1)\n",
    "        hessian: shape=(D, D)\n",
    "\n",
    "    >>> y = np.c_[[0., 1.]]\n",
    "    >>> tx = np.arange(6).reshape(2, 3)\n",
    "    >>> w = np.array([[0.1], [0.2], [0.3]])\n",
    "    >>> loss, gradient, hessian = logistic_regression(y, tx, w)\n",
    "    >>> round(loss, 8)\n",
    "    0.62137268\n",
    "    >>> gradient, hessian\n",
    "    (array([[-0.10370763],\n",
    "           [ 0.2067104 ],\n",
    "           [ 0.51712843]]), array([[0.28961235, 0.3861498 , 0.48268724],\n",
    "           [0.3861498 , 0.62182124, 0.85749269],\n",
    "           [0.48268724, 0.85749269, 1.23229813]]))\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # return loss, gradient, and Hessian: TODO\n",
    "    loss = calculate_loss(y,tx,w)\n",
    "    grad = calculate_gradient(y,tx,w)\n",
    "    #hess = calculate_hessian(y,tx,w)\n",
    "    return loss,grad#,hess\n",
    "    # ***************************************************\n",
    "    #raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "def penalized_logistic_regression(y, tx, w, lambda_):\n",
    "    \"\"\"return the loss and gradient.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1)\n",
    "        lambda_: scalar\n",
    "\n",
    "    Returns:\n",
    "        loss: scalar number\n",
    "        gradient: shape=(D, 1)\n",
    "\n",
    "    >>> y = np.c_[[0., 1.]]\n",
    "    >>> tx = np.arange(6).reshape(2, 3)\n",
    "    >>> w = np.array([[0.1], [0.2], [0.3]])\n",
    "    >>> lambda_ = 0.1\n",
    "    >>> loss, gradient = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    >>> round(loss, 8)\n",
    "    0.63537268\n",
    "    >>> gradient\n",
    "    array([[-0.08370763],\n",
    "           [ 0.2467104 ],\n",
    "           [ 0.57712843]])\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # return loss, gradient, and Hessian: TODO\n",
    "    loss,grad = logistic_regression(y, tx, w)\n",
    "    #print(loss)\n",
    "    #print('grad ',(np.shape(grad)),'lamb ', (lambda_),'w ',np.shape(np.abs(w)*2))\n",
    "    1/y.shape[0]*(-tx.T@y+tx.T@sigmoid(tx@w)+ lambda_*abs(w)*2*y.shape[0])\n",
    "    \n",
    "    grad_pen = grad+lambda_*np.abs(w)*2\n",
    "    loss_pen = loss+lambda_*np.sum(w.T@w)\n",
    "    \n",
    "    return loss,grad_pen\n",
    "    # ***************************************************\n",
    "    #raise NotImplementedError\n",
    "\n",
    "\n",
    "\n",
    "def learning_by_penalized_gradient(y, tx, w, gamma, lambda_):\n",
    "    \"\"\"\n",
    "    Do one step of gradient descent, using the penalized logistic regression.\n",
    "    Return the loss and updated w.\n",
    "\n",
    "    Args:\n",
    "        y:  shape=(N, 1)\n",
    "        tx: shape=(N, D)\n",
    "        w:  shape=(D, 1)\n",
    "        gamma: scalar\n",
    "        lambda_: scalar\n",
    "\n",
    "    Returns:\n",
    "        loss: scalar number\n",
    "        w: shape=(D, 1)\n",
    "\n",
    "    >>> np.set_printoptions(8)\n",
    "    >>> y = np.c_[[0., 1.]]\n",
    "    >>> tx = np.arange(6).reshape(2, 3)\n",
    "    >>> w = np.array([[0.1], [0.2], [0.3]])\n",
    "    >>> lambda_ = 0.1\n",
    "    >>> gamma = 0.1\n",
    "    >>> loss, w = learning_by_penalized_gradient(y, tx, w, gamma, lambda_)\n",
    "    >>> round(loss, 8)\n",
    "    0.63537268\n",
    "    >>> w\n",
    "    array([[0.10837076],\n",
    "           [0.17532896],\n",
    "           [0.24228716]])\n",
    "    \"\"\"\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # return loss, gradient: TODO\n",
    "    #loss,grad,hess = logistic_regression(y, tx, w)\n",
    "    # ***************************************************\n",
    "    #raise NotImplementedError\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # update w: TODO\n",
    "    #loss,grad,hess = logistic_regression(y, tx, w)\n",
    "    loss,grad_pen = penalized_logistic_regression(y, tx, w, lambda_)\n",
    "    w = w - gamma*(grad_pen)\n",
    "    # ***************************************************\n",
    "    #raise NotImplementedError\n",
    "    return w, loss\n",
    "\n",
    "\n",
    "\n",
    "def max_k_fold_cross_valid_sets(y,k_fold):\n",
    "    \"\"\"build k indices for k-fold.\n",
    "\n",
    "    Args:\n",
    "        y:      shape=(N,)\n",
    "        k_fold: K in K-fold, i.e. the fold num\n",
    "        seed:   the random seed\n",
    "\n",
    "    Returns:\n",
    "        A 2D array of shape=(k_fold, N/k_fold) that indicates the data indices for each fold\n",
    "\n",
    "    >>> build_k_indices(np.array([1., 2., 3., 4.]), 2, 1)\n",
    "    array([[3, 2],\n",
    "           [0, 1]])\n",
    "    \"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    #np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval : (k + 1) * interval] for k in range(k_fold)]\n",
    "    #print(np.array(k_indices))\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def max_cross_valid(y, x, k_fold, initial_w,max_iter,gamma,lambda_, regressionFunction=learning_by_penalized_gradient,lossFunction=calculate_loss):\n",
    "    \"\"\"return the loss of ridge regression for a fold corresponding to k_indices\n",
    "\n",
    "    Args:\n",
    "        y:          shape=(N,)\n",
    "        x:          shape=(N,)\n",
    "        k_indices:  2D array returned by build_k_indices()\n",
    "        k:          scalar, the k-th fold (N.B.: not to confused with k_fold which is the fold nums)\n",
    "        lambda_:    scalar, cf. ridge_regression()\n",
    "        degree:     scalar, cf. build_poly()\n",
    "\n",
    "    Returns:\n",
    "        train and test root mean square errors rmse = sqrt(2 mse)\n",
    "\n",
    "    >>> cross_validation(np.array([1.,2.,3.,4.]), np.array([6.,7.,8.,9.]), np.array([[3,2], [0,1]]), 1, 2, 3)\n",
    "    (0.019866645527597114, 0.33555914361295175)\n",
    "    \"\"\"\n",
    "\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # get k'th subgroup in test, others in train: TODO\n",
    "    loss_train_test = []\n",
    "    loss_tr_arr = []\n",
    "    loss_te_arr = []\n",
    "    for k in range(k_fold):\n",
    "        k_indices = max_k_fold_cross_valid_sets(y, k_fold)\n",
    "        #print(np.shape(k_indices))\n",
    "        test_ind = k_indices[k]\n",
    "        train_ind = (k_indices[np.arange(len(k_indices))!=k]).flatten()\n",
    "\n",
    "        x_train = x[train_ind]\n",
    "        x_test = x[test_ind]\n",
    "\n",
    "        y_train = y[train_ind].flatten()\n",
    "        y_test = y[test_ind]\n",
    "        #print('x_train: ', np.shape(x_train),'y_train: ',np.shape(y_train))\n",
    "\n",
    "        if regressionFunction==reg_logistic_regression:\n",
    "            weights, loss_train = regressionFunction(y_train, x_train,lambda_, initial_w, max_iter, gamma)\n",
    "            \n",
    "        else:\n",
    "            weights, loss_train = regressionFunction(y_train, x_train, initial_w, max_iter, gamma)\n",
    "            \n",
    "        #print(weights)\n",
    "        loss_tr_test = lossFunction(y_train, x_train, weights)\n",
    "        loss_test = lossFunction(y_test, x_test, weights)\n",
    "       \n",
    "        loss_train_test.append(loss_tr_test)\n",
    "        loss_tr_arr.append(loss_train/k_fold)\n",
    "        loss_te_arr.append(loss_test/k_fold)\n",
    "        print(f'Run {k+1} yielded a loss improvement from {loss_train} to {loss_test}')\n",
    "        print('______________________')\n",
    "    \n",
    "    \n",
    "  \n",
    "    return loss_tr_arr,loss_te_arr,loss_train_test\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b700316c-c8d6-4f45-8ea6-8441b0a39fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
